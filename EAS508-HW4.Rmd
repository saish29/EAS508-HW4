---
title: "EAS508-HW4"
author: "Saish Mandavkar"
date: "2022-10-20"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Lab Code Homework

## 5.3 Cross Validation Labs

### 5.3.1 Validation Set Approach

```{r}

# Setting the seed and loading the data

library(ISLR2)
set.seed(1)
train <- sample(392,196)

```

```{r}

# Fitting a linear regression on the train data using subset option

lm.fit <- lm(mpg ~ horsepower, data = Auto, subset = train)
```

```{r}

# Predicting the estimates for the 392 observations and calculate the MSE for 192 observations 

mean((Auto$mpg - predict(lm.fit, Auto))[-train]^2)
```

```{r}

# Fitting cubic regression and calculating the MSE

lm.fit2 <- lm(mpg ~poly(horsepower, 2), data = Auto, subset = train)

mean((Auto$mpg - predict(lm.fit2, Auto))[-train]^2)
```

```{r}

# Fitting uadratic regression and calculating the MSE

lm.fit3 <- lm(mpg ~ poly(horsepower, 3), data = Auto, subset = train)

mean((Auto$mpg - predict(lm.fit3, Auto))[-train]^2)
```

```{r}

# Using different seed and calculatiing the values for all the three regressions - will result into different MSE values.

set.seed(2)
train <- sample(392,196)

# Linear regression MSE 

lm.fit <- lm(mpg ~ horsepower, data = Auto, subset = train)

mean((Auto$mpg - predict(lm.fit, Auto))[-train]^2) 

# Cubic regression MSE

lm.fit2 <- lm(mpg ~poly(horsepower, 2), data = Auto, subset = train)

mean((Auto$mpg - predict(lm.fit2, Auto))[-train]^2)

# Quadratic regression MSE


lm.fit3 <- lm(mpg ~poly(horsepower, 3), data = Auto, subset = train)

mean((Auto$mpg - predict(lm.fit3, Auto))[-train]^2)

```

### 5.3.2 Leave One-Out Cross-Validation

```{r}

# LOOCV using glm() package

glm.fit <- glm(mpg ~ horsepower, data = Auto)

coef(glm.fit)
```

```{r}

# LOOCV using normal lm() function

lm.fit <- lm(mpg ~ horsepower, data = Auto)

coef(lm.fit)
```

```{r}

# Cross-validation error using glm() package

library(boot)

glm.fit <- glm(mpg ~ horsepower, data = Auto)

cv.err <- cv.glm(Auto, glm.fit)

cv.err$delta
```

```{r}

# Calculating CV error for for polynomial of order 1 to 10 using a for loop.

cv.error <- rep(0,10)

for (i in  1:10) {
  
  glm.fit <- glm(mpg ~ poly(horsepower, i), data = Auto)
  cv.error[i] <- cv.glm(Auto, glm.fit)$delta[1]
  
}

cv.error
```

### 5.3.3 k-Fold Cross Validation

```{r}

# Calculating k-fold CV error for for polynomial of order 1 to 10 with k = 10

set.seed(17)
cv.error.10 <- rep(0,10)

for (i in  1:10) {
  
  glm.fit <- glm(mpg ~ poly(horsepower, i), data = Auto)
  cv.error.10[i] <- cv.glm(Auto, glm.fit, K = 10)$delta[1]
  
}

cv.error.10

```

## 6.5.3 PCR and PLS Regression

#### Principal Components Regression

```{r}

# Creating model matrix for x and storing all salary values in y

# Omitting NA values 

Hitters <- na.omit(Hitters)

x <- model.matrix(Salary ~ ., Hitters)[, -1]
y <- Hitters$Salary

# Creating train and test by setting the R seed

set.seed(1)
train <- sample(1:nrow(x), nrow(x) / 2)
test <- (-train)
y.test <- y[test]
```

```{r}

# Applying PCR to Hitters data to predcit Salary

library(pls)
set.seed(2)
pcr.fit <- pcr(Salary ~., data = Hitters, scale = TRUE, validation = "CV")
```

```{r}

# Checking summary of our fit

summary(pcr.fit)
```

```{r}

# Plotting cross-validation MSE 

validationplot(pcr.fit, val.type = "MSEP")
```

```{r}

# Performing PCR on the training data using subset function and plotting the CV MSE

set.seed(1)

pcr.fit <- pcr(Salary ~., data = Hitters, subset = train, scale = TRUE, validation = "CV")

validationplot(pcr.fit, val.type = "MSEP")
```

```{r}

# Find the lowest CV error when M = 5


pcr.pred <- predict(pcr.fit, x[test, ], ncomp = 5)

mean((pcr.pred - y.test)^2)

```

```{r}

# Fit PCR on complete dataset using M = 5 identified by CV

pcr.fit <- pcr(y~x, scale = TRUE, ncomp = 5)

summary(pcr.fit)
```

#### Partial Least Squares

```{r}

# Implement PLS using plsr() function 

set.seed(1)
pls.fit <- plsr(Salary ~ ., data = Hitters, subset = train, scale = TRUE, validation = "CV")

summary(pls.fit)
```

```{r}

# Evaluating the coresponding test set MSE

pls.pred <- predict(pls.fit, x[test, ], ncomp = 1)

mean((pls.pred - y.test)^2)
```

```{r}

# Fitting PLS on complete dataset when M = 1 

pls.fit <- plsr(Salary ~ ., data = Hitters, scale = TRUE, ncomp = 1)

summary(pls.fit)
```

### 7.8.3 GAMs

```{r}

# Fit a GAM or predict wage using natural spline functions of years and age.

gam1 <- lm(wage ~ splines::ns(year, 4) + splines::ns(age, 5) + education, data = Wage)
```

```{r}

# Fit the model using smoothing splines

library(gam)

gam.m3 <- gam(wage ~ s(year, 4) + s(age, 5) + education, data = Wage)
```

```{r}

# Plot the model

par(mfrow = c(1,3))
plot(gam.m3, se = TRUE, col = "blue")
```

```{r}

# Plotting the GAM created using lm 

par(mfrow = c(1,3))
plot.Gam(gam1, se = TRUE, col = "red")
```

```{r}

# Performing ANOVA test to determine the best model 

gam.m1 <- gam(wage ~ s(age, 5) + education,  data = Wage)
gam.m2 <- gam(wage ~ year + s(age, 5) + education, data = Wage)

anova(gam.m1, gam.m2, gam.m3, test = "F")
```

```{r}

# summary of gam.m3

summary(gam.m3)
```

```{r}

# Using the predict method for class GAM

preds <- predict(gam.m2, newdata = Wage)
```

```{r}

# Using local regression fits in GAM using lo()

gam.lo <- gam(wage ~ s(year, df = 4) + lo(age, span = 0.7) + education, data = Wage)

par(mfrow = c(1,3))
plot.Gam(gam.lo, se = TRUE, col = "green")
```

```{r}

# Using lo() to create interactions before calling gam

gam.lo.i <- gam(wage ~ lo(year, age, span = 0.5) + education, data = Wage)
```

```{r}

# Plotting the 2D surface using akima package

library(akima)
par(mfrow = c(1,2))
plot(gam.lo.i)
```

```{r}

# Fittinga logistic regression GAM using I() function

gam.lr <- gam(I(wage > 250) ~ year + s(age, df = 5) + education, family = binomial, data = Wage)

par(mfrow = c(1,3))
plot(gam.lr, se = TRUE, col = "green")
```

```{r}

# Creeate a table with high earnes in the < HS category

attach(Wage)
table(education, I(wage > 250))
```

```{r}

# Fitting a logsitic regression GAM by skipping the education category

gam.lr.s <- gam( I(wage > 250) ~ year + s(age, df = 5) + education, family = binomial, subset = (education != "1. < HS Grad"))

par(mfrow = c(1,3))
plot(gam.lr.s, se = TRUE, col = "green")
```

## 5.4 Q8 - Perform cross-validation on a simulated data set.

**(a) Generate a simulated data set as follows:**

```{r}

# Generate a simulated data set

set.seed(1)

x <- rnorm(100)
y <- x - 2 * x^2 + rnorm(100)
```

As we have one predictor coefficient and 100 observations (as rnorm(100) is used) , our **n = 1** and **p = 100**

Our equation will be ***Y = x - 2x\^2 + e***

**(b) Create a scatterplot of X against Y. Comment on what you find.**

```{r}

# Scatterplot of X against Y. 



myfun <- function(x) {
  x - 2 * x^2
  
}

# Using ggplot2 to plot a scatterplot

library(ggplot2)

ggplot(mapping  = aes(x = x,y = y)) +
  geom_point(colour = "blue") + 
  geom_smooth(method = "lm", formula = "y ~ x + I(x^2)", se = FALSE, colour = "orange")
  
  
# 
```

As we can see, our equation creates a concave down parabola and and we can notice a little bit of noise at the vertex of our parabola. The points with the support of our curve also suggests the existence of a curved relationship which hints to a quadratic relationship.

**(c) Set a random seed, and then compute then LOOCV errors that result from fitting the following four models using least squares:**

As we can see from all the models, they start from polynomial order 1 to order 4, so we can use a loop to compute all the LOOCV errors.

Let's create a data frame out of our x and y values.

```{r}

# Creating a data frame using random seed

set.seed(25)

data <- data.frame(x = x, y = y)
```

```{r}

# Computing LOOCV errors from polynomial order 1 to 4. 

loocv.error <- rep(0,4)

for (i in  1:4) {
  
  glm.fit <- glm(y ~ poly(x, i), data = data)
  loocv.error[i] <- cv.glm(data, glm.fit)$delta[1]
  
}

poly_order <- c("Order 1", "Order 2", "Order 3", "Order 4")


setNames(loocv.error, poly_order)
```

**(d) Repeat (c) using another random seed, and report your results. Are your results the same as what you got in (c)? Why?**

```{r}

# Changing seed and repeating the steps

set.seed(50)

loocv.error <- rep(0,4)

for (i in  1:4) {
  
  glm.fit <- glm(y ~ poly(x, i), data = data)
  loocv.error[i] <- cv.glm(data, glm.fit)$delta[1]
  
}

poly_order <- c("Order 1", "Order 2", "Order 3", "Order 4")


setNames(loocv.error, poly_order)

```

We get exact same error values with a random seed and this is because in LOOCV there is no element of randomness i.e. we will always have the same LOOCV error values every time.

**(e) Which of the models in (c) had the smallest LOOCV error? Is that what you expected? Explain your answer.**

We see that the the lowest value of LOOCV error is at Order 2 which is **0.937** which is not much surprising to see as the scatter plot which we created in (b) already hinted to a quadratic relationship.

**(f) Comment on the statistical significance of the coefficient estimates that results from fitting each of the models in (c) using least squares. Do these results agree with the conclusions drawn based on the cross-validation results ?**

```{r}

# Looking at coefficients significants for all the order models, can be seen in order 4 polynomial regression

glm.fit4 <- glm(y ~ poly(x, 4), data = data)

summary(glm.fit)


```

## 6.6 Q9 - Predict the number of applications received using the other variables in the `College` data set. 

**(a) Split the data set into a training set and a test set.**

```{r}

# Cleaning data and splitting the dataset into train and test data 

# Loading the necessary packages

library(ISLR)
library(glmnet)
library(pls)

# Removing NA values

College <- na.omit(College)


set.seed(49)

train_index <- sample(1:dim(College)[1], dim(College)[1] / 2)
test_index <- (-train_index)

train <- College[train_index, ]
test <- College[test_index, ]

train
```

**(b) Fit a linear model using least squares on the training set, and report the test error obtained.**

```{r}

# Fitting a linear model 

lm.fit <- lm(Apps ~ ., data = train)

# Predicting the values

lm.pred <- predict(lm.fit, test)

lm.mse <- mean((lm.pred - test$Apps)^2)

lm.mse
```
