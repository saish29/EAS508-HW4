---
title: "EAS508-HW4"
author: "Saish Mandavkar"
date: "2022-10-20"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Lab Code Homework

## 5.3 Cross Validation Labs

### 5.3.1 Validation Set Approach

```{r}

# Setting the seed and loading the data

library(ISLR2)
set.seed(1)
train <- sample(392,196)

```

```{r}

# Fitting a linear regression on the train data using subset option

lm.fit <- lm(mpg ~ horsepower, data = Auto, subset = train)
```

```{r}

# Predicting the estimates for the 392 observations and calculate the MSE for 192 observations 

mean((Auto$mpg - predict(lm.fit, Auto))[-train]^2)
```

```{r}

# Fitting cubic regression and calculating the MSE

lm.fit2 <- lm(mpg ~poly(horsepower, 2), data = Auto, subset = train)

mean((Auto$mpg - predict(lm.fit2, Auto))[-train]^2)
```

```{r}

# Fitting uadratic regression and calculating the MSE

lm.fit3 <- lm(mpg ~ poly(horsepower, 3), data = Auto, subset = train)

mean((Auto$mpg - predict(lm.fit3, Auto))[-train]^2)
```

```{r}

# Using different seed and calculatiing the values for all the three regressions - will result into different MSE values.

set.seed(2)
train <- sample(392,196)

# Linear regression MSE 

lm.fit <- lm(mpg ~ horsepower, data = Auto, subset = train)

mean((Auto$mpg - predict(lm.fit, Auto))[-train]^2) 

# Cubic regression MSE

lm.fit2 <- lm(mpg ~poly(horsepower, 2), data = Auto, subset = train)

mean((Auto$mpg - predict(lm.fit2, Auto))[-train]^2)

# Quadratic regression MSE


lm.fit3 <- lm(mpg ~poly(horsepower, 3), data = Auto, subset = train)

mean((Auto$mpg - predict(lm.fit3, Auto))[-train]^2)

```

### 5.3.2 Leave One-Out Cross-Validation

```{r}

# LOOCV using glm() package

glm.fit <- glm(mpg ~ horsepower, data = Auto)

coef(glm.fit)
```

```{r}

# LOOCV using normal lm() function

lm.fit <- lm(mpg ~ horsepower, data = Auto)

coef(lm.fit)
```

```{r}

# Cross-validation error using glm() package

library(boot)

glm.fit <- glm(mpg ~ horsepower, data = Auto)

cv.err <- cv.glm(Auto, glm.fit)

cv.err$delta
```

```{r}

# Calculating CV error for for polynomial of order 1 to 10 using a for loop.

cv.error <- rep(0,10)

for (i in  1:10) {
  
  glm.fit <- glm(mpg ~ poly(horsepower, i), data = Auto)
  cv.error[i] <- cv.glm(Auto, glm.fit)$delta[1]
  
}

cv.error
```

### 5.3.3 k-Fold Cross Validation

```{r}

# Calculating k-fold CV error for for polynomial of order 1 to 10 with k = 10

set.seed(17)
cv.error.10 <- rep(0,10)

for (i in  1:10) {
  
  glm.fit <- glm(mpg ~ poly(horsepower, i), data = Auto)
  cv.error.10[i] <- cv.glm(Auto, glm.fit, K = 10)$delta[1]
  
}

cv.error.10

```

## 6.5.3 PCR and PLS Regression

#### Principal Components Regression

```{r}

# Creating model matrix for x and storing all salary values in y

# Omitting NA values 

Hitters <- na.omit(Hitters)

x <- model.matrix(Salary ~ ., Hitters)[, -1]
y <- Hitters$Salary

# Creating train and test by setting the R seed

set.seed(1)
train <- sample(1:nrow(x), nrow(x) / 2)
test <- (-train)
y.test <- y[test]
```

```{r}

# Applying PCR to Hitters data to predcit Salary

library(pls)
set.seed(2)
pcr.fit <- pcr(Salary ~., data = Hitters, scale = TRUE, validation = "CV")
```

```{r}

# Checking summary of our fit

summary(pcr.fit)
```

```{r}

# Plotting cross-validation MSE 

validationplot(pcr.fit, val.type = "MSEP")
```

```{r}

# Performing PCR on the training data using subset function and plotting the CV MSE

set.seed(1)

pcr.fit <- pcr(Salary ~., data = Hitters, subset = train, scale = TRUE, validation = "CV")

validationplot(pcr.fit, val.type = "MSEP")
```

```{r}

# Find the lowest CV error when M = 5


pcr.pred <- predict(pcr.fit, x[test, ], ncomp = 5)

mean((pcr.pred - y.test)^2)

```

```{r}

# Fit PCR on complete dataset using M = 5 identified by CV

pcr.fit <- pcr(y~x, scale = TRUE, ncomp = 5)

summary(pcr.fit)
```

#### Partial Least Squares

```{r}

# Implement PLS using plsr() function 

set.seed(1)
pls.fit <- plsr(Salary ~ ., data = Hitters, subset = train, scale = TRUE, validation = "CV")

summary(pls.fit)
```

```{r}

# Evaluating the coresponding test set MSE

pls.pred <- predict(pls.fit, x[test, ], ncomp = 1)

mean((pls.pred - y.test)^2)
```

```{r}

# Fitting PLS on complete dataset when M = 1 

pls.fit <- plsr(Salary ~ ., data = Hitters, scale = TRUE, ncomp = 1)

summary(pls.fit)
```

### 7.8.3 GAMs

```{r}

# Fit a GAM or predict wage using natural spline functions of years and age.

gam1 <- lm(wage ~ splines::ns(year, 4) + splines::ns(age, 5) + education, data = Wage)
```

```{r}

# Fit the model using smoothing splines

library(gam)

gam.m3 <- gam(wage ~ s(year, 4) + s(age, 5) + education, data = Wage)
```

```{r}

# Plot the model

par(mfrow = c(1,3))
plot(gam.m3, se = TRUE, col = "blue")
```

```{r}

# Plotting the GAM created using lm 

par(mfrow = c(1,3))
plot.Gam(gam1, se = TRUE, col = "red")
```

```{r}

# Performing ANOVA test to determine the best model 

gam.m1 <- gam(wage ~ s(age, 5) + education,  data = Wage)
gam.m2 <- gam(wage ~ year + s(age, 5) + education, data = Wage)

anova(gam.m1, gam.m2, gam.m3, test = "F")
```

```{r}

# summary of gam.m3

summary(gam.m3)
```

```{r}

# Using the predict method for class GAM

preds <- predict(gam.m2, newdata = Wage)
```

```{r}

# Using local regression fits in GAM using lo()

gam.lo <- gam(wage ~ s(year, df = 4) + lo(age, span = 0.7) + education, data = Wage)

par(mfrow = c(1,3))
plot.Gam(gam.lo, se = TRUE, col = "green")
```

```{r}

# Using lo() to create interactions before calling gam

gam.lo.i <- gam(wage ~ lo(year, age, span = 0.5) + education, data = Wage)
```

```{r}

# Plotting the 2D surface using akima package

library(akima)
par(mfrow = c(1,2))
plot(gam.lo.i)
```

```{r}

# Fittinga logistic regression GAM using I() function

gam.lr <- gam(I(wage > 250) ~ year + s(age, df = 5) + education, family = binomial, data = Wage)

par(mfrow = c(1,3))
plot(gam.lr, se = TRUE, col = "green")
```

```{r}

# Creeate a table with high earnes in the < HS category

attach(Wage)
table(education, I(wage > 250))
```

```{r}

# Fitting a logsitic regression GAM by skipping the education category

gam.lr.s <- gam( I(wage > 250) ~ year + s(age, df = 5) + education, family = binomial, subset = (education != "1. < HS Grad"))

par(mfrow = c(1,3))
plot(gam.lr.s, se = TRUE, col = "green")
```
